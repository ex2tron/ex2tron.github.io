<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
  

  <!-- PACE Progress Bar START -->

  
  <title>小白深度学习笔记3：浅层神经网络 | ex2tron&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="神经网络" />
  
  
  
  
  <meta name="description" content="logistic回归可以看作是只有1层1个神经元的神经网络，那么一般的神经网络是怎么样的呢？">
<meta name="keywords" content="神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="小白深度学习笔记3：浅层神经网络">
<meta property="og:url" content="http://ex2tron.coding.me/2017/10/13/小白深度学习笔记3：浅层神经网络/index.html">
<meta property="og:site_name" content="ex2tron&#39;s Blog">
<meta property="og:description" content="logistic回归可以看作是只有1层1个神经元的神经网络，那么一般的神经网络是怎么样的呢？">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://pic.ex2tron.top/logistic_regression_singal_neural_network_sample.jpg">
<meta property="og:image" content="http://pic.ex2tron.top/two_layers_neural_network_samples.jpg">
<meta property="og:image" content="http://pic.ex2tron.top/common_activation_functions.jpg">
<meta property="og:image" content="http://pic.ex2tron.top/no_zero_initialized_sample.jpg">
<meta property="og:updated_time" content="2017-12-20T09:20:00.144Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小白深度学习笔记3：浅层神经网络">
<meta name="twitter:description" content="logistic回归可以看作是只有1层1个神经元的神经网络，那么一般的神经网络是怎么样的呢？">
<meta name="twitter:image" content="http://pic.ex2tron.top/logistic_regression_singal_neural_network_sample.jpg">
  
    <link rel="alternate" href="/atom.xml" title="ex2tron&#39;s Blog" type="application/atom+xml">
  
  <link rel="icon" href="//img/bookshelf.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/hiero.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >
  
    <link rel="stylesheet" href="/css/vdonate.css" >
  

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/my.css">

</head>

<script>
var themeMenus = {};

  themeMenus["/"] = "Home"; 

  themeMenus["/archives"] = "Archives"; 

  themeMenus["/categories"] = "Categories"; 

  themeMenus["/tags"] = "Tags"; 

  themeMenus["/about"] = "About"; 

</script>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="ex2tron&#39;s Blog" rel="home"> ex2tron&#39;s Blog </a>
            
          </h1>

          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">Tags</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">About</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>




  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-小白深度学习笔记3：浅层神经网络" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      小白深度学习笔记3：浅层神经网络
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2017/10/13/小白深度学习笔记3：浅层神经网络/" class="article-date">
	  <time datetime="2017-10-13T07:15:18.000Z" itemprop="datePublished">October 13, 2017</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>logistic回归可以看作是只有1层1个神经元的神经网络，那么一般的神经网络是怎么样的呢？<a id="more"></a></p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p><em>因为国民不富裕就不能受法律保护，就不能享受民主，这种说法我是无法接受的。</em>  ——<a href="https://movie.douban.com/subject/21937445/" target="_blank" rel="external">《辩护人》</a></p>
</blockquote>
<p>本文是我在观看吴恩达老师的<a href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="external">《神经网络和深度学习》</a>视频课程时，相关的笔记整理，大佬勿喷！</p>
<hr>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>前面学习了logistic回归，可以把logistic看作只有1层1个神经元的神经网络，如下图：</p>
<p><img src="http://pic.ex2tron.top/logistic_regression_singal_neural_network_sample.jpg" alt="logistic_regression_singal_neural_network_sample"></p>
<p>这个神经元做两个操作：</p>
<ol>
<li>计算\(z=w^Tx+b\)</li>
<li>计算\(\hat{y}=a=\sigma(z)\)</li>
</ol>
<p>而一般的神经网路包括输入层、隐藏层和输出层。如下图所示，是一个两层神经网络。层数从隐藏层(Hidden Layer)开始算，也就是这个图中，第1层是隐藏层，第2层是输出层(Output Layer)。可以把输入层(Input Layer)称为第0层。</p>
<p><img src="http://pic.ex2tron.top/two_layers_neural_network_samples.jpg" alt="two_layers_neural_network_samples"></p>
<p>浅层神经网络是相较于深层神经网络而言的，深层神经网络中，隐藏层的个数&gt;=2，层数越多，也就越深。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>如果把2层的神经网络看作是logistic回归的堆叠的话，那么这个神经网络的正向传播应该是下面这样的公式：</p>
<p>$$z^{[1]}=W^{[1]}x+b^{[1]}\tag{1}$$<br>$$a^{[1]}=\sigma(z^{[1]})\tag{2}$$<br>$$z^{[2]}=W^{[2]}x+b^{[2]}\tag{3}$$<br>$$a^{[2]}=\sigma(z^{[2]})\tag{4}$$</p>
<p>其中，sigmoid函数就是激活函数(Activation Function)。激活函数不一定是sigmoid函数，有的非线性函数要比sigmoid函数更好，这里介绍两种：tanh和ReLU。</p>
<p><img src="http://pic.ex2tron.top/common_activation_functions.jpg" alt="common_activation_functions"></p>
<h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><p><a href="https://baike.baidu.com/item/tanh" target="_blank" rel="external">tanh</a>(Hyperbolic Tangent Function)双曲正切函数，\(a=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\)。tanh往往比sigmoid表现更好，从上图中可以看出，这个激活函数的平均值更接近于0。但是输出层如果是二类分类的话，结果要么是0要么是1，那么我们预测值应该介于0~1之间，所以这种情况下，输出层的激活函数依然用sigmoid函数。</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>如果仔细观察sigmoid和tanh函数曲线的话，就会发现，自变量z比较大或比较小时，这个函数的斜率就会很小，接近于0，这样会拖慢梯度下降法的速度。所以，另外一个经常使用到的激活函数是<a href="https://baike.baidu.com/item/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0" target="_blank" rel="external">ReLU</a>(Rectified Linear Unit)。\(a=max(0,z)\)，z为正时，导数为1，z为负时，导数为0。</p>
<p>所以，总结三种激活函数，</p>
<ul>
<li>一般不使用sigmoid做激活函数，除非是二类分类，可用在输出层</li>
<li>tanh普遍效果要比sigmoid更好</li>
<li>最常用的默认激活函数是ReLU</li>
</ul>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>如果我们选择tanh作为激活函数的话，开头那个2层神经网络的正向传播就是：</p>
<p>$$Z^{[1]}=W^{[1]}X+b^{[1]}\tag{1}$$<br>$$A^{[1]}=tanh(Z^{[1]})\tag{2}$$<br>$$Z^{[2]}=W^{[2]}X+b^{[2]}\tag{3}$$<br>$$A^{[2]}=tanh(Z^{[2]})\tag{4}$$</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>反向传播的推导跟logistic回归一样，只不过进行两次：</p>
<p>$$dZ^{[2]}=A^{[2]}-Y\tag{1}$$<br>$$dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]^T}\tag{2}$$<br>$$db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)\tag{3}$$<br>$$dZ^{[1]}=W^{[2]^T}dZ^{[2]}*tanh^{’[1]}(Z^{[1]})\tag{4}$$<br>$$dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T\tag{5}$$<br>$$db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)\tag{6}$$</p>
<h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>之前在logistic回归中，我们可以将权重W初始化为0，但是对于神经网络而言，W都初始化为0的话，是无效的。</p>
<p><img src="http://pic.ex2tron.top/no_zero_initialized_sample.jpg" alt="no_zero_initialized_sample"></p>
<p>举例来说，对于上图中的神经网络，隐藏层神经元个数是2，如果W全部初始化为0的话，那么：</p>
<p>$$<br>    W^{[1]}=<br>    \begin{bmatrix}<br>    0 &amp; 0 \newline<br>    0 &amp; 0<br>    \end{bmatrix}<br>$$</p>
<p>因为\(z^{[1]}=W^{[1]}x+b\)，这样的话\(z_1^{[1]}=z_2^{[1]}\)，\(a_1^{[1]}=a_2^{[1]}\)，也就是说，两个神经元的功能完全一样，计算相同的值，这样1个神经元和n个神经元对神经网络的作用是一样的，其他n-1个是多余的。</p>
<p>所以，在神经网络的参数初始化中，权重W是不能全部初始化为0的，b的话无所谓，都可以。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/深度学习/">深度学习</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>

      
        <div id="donation_div"></div>

<script src="/js/vdonate.js"></script>
  <script>
    var a = new Donate({
      title: '谢谢支持，我会更加✊~', // 可选参数，打赏标题
      // btnText: 'Donate', // 可选参数，打赏按钮文字
      btnText: '赏', // 可选参数，打赏按钮文字
      el: document.getElementById('donation_div'),
      wechatImage: '/img/wechat2.jpg',
      alipayImage: '/img/alipay.jpg'
    });
  </script>
      
            
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>


      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/11/08/摄影之魅：瞬间是一面镜子/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          摄影之魅：瞬间是一面&#34;镜子&#34;
        
      </div>
    </a>
  
  
    <a href="/2017/10/11/numpy中几种矩阵的乘法/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">numpy中几种矩阵的乘法</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络"><span class="nav-number">1.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-number">2.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh"><span class="nav-number">2.1.</span> <span class="nav-text">tanh</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ReLU"><span class="nav-number">2.2.</span> <span class="nav-text">ReLU</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正向传播"><span class="nav-number">3.</span> <span class="nav-text">正向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">4.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机初始化"><span class="nav-number">5.</span> <span class="nav-text">随机初始化</span></a></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>
      <footer id="footer" class="site-footer">
  

        <div class="clearfix container">
          <div class="site-info">
            &copy;
            2018
              ex2tron&#39;s Blog All Rights Reserved.
                
          </div>
          <!-- <div class="site-credit">
            Theme by
              <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
          </div> -->
          <div>
            <p>Hosted by
              <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a>
            </p>
          </div>
        </div>
</footer>


<!-- min height -->

<script>
  var contentdiv = document.getElementById("content");

  contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->
<script src="/js/my.js"></script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/scripts.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>








  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
